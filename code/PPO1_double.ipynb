{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, torch, numpy as np, torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from copy import deepcopy\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Independent, Normal\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from env import OilControlEnv\n",
    "from common.tools import load_json_config, load_sys_config\n",
    "from common.utils import *\n",
    "from common.log_path import make_logpath\n",
    "\n",
    "from solver.gurobi.solve import solve as gurobi_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "强化学习+求解器模型\n",
    "强化学习算法为ppo\n",
    "算法库thu-tianshou\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47250eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'OilSupply'\n",
    "lr, epoch, batch_size = 1e-5, 20, 1024\n",
    "train_num, test_num = 1, 1\n",
    "gamma, lr_decay = 0.9, None\n",
    "buffer_size = 1000000\n",
    "buffer_alpha, buffer_beta = 0.6, 0.4\n",
    "eps_train, eps_test = 0.1, 0.00\n",
    "step_per_epoch, episode_per_collect = 30*train_num*500, train_num\n",
    "writer = SummaryWriter('log/ppo1')  # tensorboard is also supported!\n",
    "logger = ts.utils.BasicLogger(writer)\n",
    "is_gpu = True\n",
    "#ppo\n",
    "gae_lambda, max_grad_norm = 0.95, 0.5\n",
    "vf_coef, ent_coef = 0.25, 0.0\n",
    "rew_norm, action_scaling = False, False\n",
    "bound_action_method = \"clip\"\n",
    "eps_clip, value_clip = 0.2, False\n",
    "repeat_per_collect = 2\n",
    "dual_clip, norm_adv = None, 0.0\n",
    "recompute_adv = 0\n",
    "\n",
    "solver_reward_k = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfacaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_dir = \"./config\"\n",
    "env_configs = load_config(env_config_dir, 'oil_env')\n",
    "env_args = get_paras_from_dict(env_configs)\n",
    "env_all_conf = load_json_config(\"env/config.json\")\n",
    "env_conf = env_all_conf['Oil_Control']\n",
    "env_sys_conf = load_sys_config(env_args.config_path, env_args.model_id)\n",
    "env_run_dir, env_log_dir = make_logpath(env_args.scenario, env_args.algo)\n",
    "\n",
    "class OilSupply_Env():\n",
    "    def __init__(self):\n",
    "        self.env = OilControlEnv(env_conf, env_sys_conf)\n",
    "        self.reset()\n",
    "        self.action_space = self.env.action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_cnt = 0\n",
    "        self.state = self.env.reset()\n",
    "        obs = np.array(self.env.obs2vec(self.state))\n",
    "        self.obs_space = len(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, vec_action):\n",
    "        self.step_cnt += 1\n",
    "        gurobi_action = self.RL_action2dict(vec_action)\n",
    "        dict_action, solver_obj = gurobi_solver(self.env.vertices, self.env.edges, gurobi_action, self.step_cnt, False)  \n",
    "        self.state, reward, done, info = self.env.step(dict_action)\n",
    "        obs = np.array(self.env.obs2vec(self.state))\n",
    "        reward = reward*2 + 15.35\n",
    "#         print(dict_action)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def RL_action2dict(self, action):\n",
    "        action_dict = {}\n",
    "        action_dict = {}\n",
    "        idx = 0\n",
    "        vec = action.tolist()\n",
    "        for key in self.state.keys():\n",
    "            if key == \"transfer\":\n",
    "                action_dict['transfer'] = []\n",
    "                for item in self.state[key]:\n",
    "                    data = {}\n",
    "                    data['key'] = item['key']\n",
    "                    data['storage'] = dict(zip(item['materials'][1:], vec[idx:idx+len(item[\"storage\"])-1]))\n",
    "                    # data['storage'] = vec[idx:idx+len(item[\"storage\"])]\n",
    "                    idx += len(item['storage']) - 1\n",
    "                    action_dict['transfer'].append(data)\n",
    "            if key == \"refinery\":\n",
    "                action_dict['refinery'] = []\n",
    "                for item in self.state[key]:\n",
    "                    data = {}\n",
    "                    data['key'] = item['key']\n",
    "                    # if isinstance(item[\"left_JG_budget\"], list):\n",
    "                    #     data['left_JG_budget'] = vec[idx:idx+len(item[\"left_JG_budget\"])]\n",
    "                    #     idx += len(item[\"left_JG_budget\"])\n",
    "                    # if isinstance(item[\"left_JG_budget\"], int):\n",
    "                    #     data['left_JG_budget'] = {'JGHY': vec[idx]}\n",
    "                    #     idx += 1\n",
    "                    data['storage'] = {}\n",
    "                    for j in ['JGHY', \"PGLE\", \"PLDO\"]:\n",
    "                        if j in item[\"storage\"].keys():\n",
    "                            data['storage'][j] = vec[idx]\n",
    "                            idx += 1\n",
    "                    action_dict['refinery'].append(data)\n",
    "#         print(idx)\n",
    "        return action_dict\n",
    "\n",
    "\n",
    "\n",
    "sample_env = OilSupply_Env()\n",
    "obs_space = sample_env.obs_space\n",
    "action_space = sample_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_resblock_relu(nn.Module):\n",
    "    def __init__(self, in_ch, ch, out_ch=None, block_num=3, is_relu=True):\n",
    "        super().__init__()\n",
    "        self.models=nn.Sequential()\n",
    "        self.relus=nn.Sequential()\n",
    "        self.block_num = block_num\n",
    "        self.is_in = in_ch\n",
    "        self.is_out = out_ch\n",
    "        self.is_relu = is_relu\n",
    "        \n",
    "        if self.is_in:\n",
    "            self.in_mlp = nn.Sequential(*[\n",
    "                nn.Linear(in_ch, ch), \n",
    "                nn.LeakyReLU(0.1, inplace=True)])\n",
    "        for i in range(self.block_num):\n",
    "            self.models.add_module(str(i), nn.Sequential(*[\n",
    "                nn.Linear(ch, ch),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Linear(ch, ch)]))\n",
    "            self.relus.add_module(str(i), nn.Sequential(*[\n",
    "                nn.LeakyReLU(0.1, inplace=True)]))\n",
    "        if self.is_out:\n",
    "            self.out_mlp = nn.Sequential(*[\n",
    "            nn.Linear(ch, ch), \n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Linear(ch, out_ch)\n",
    "            ])\n",
    "        if self.is_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.is_in:\n",
    "            x = self.in_mlp(x)\n",
    "        for i in range(self.block_num):\n",
    "            x0 = x\n",
    "            x = self.models[i](x)\n",
    "            x += x0\n",
    "            x = self.relus[i](x)\n",
    "        if self.is_out:\n",
    "            x = self.out_mlp(x)\n",
    "        if self.is_relu:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "MLP_CH = 1024\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, is_gpu=True):\n",
    "        super().__init__()\n",
    "        self.is_gpu = is_gpu\n",
    "        self.net = mlp_resblock_relu(in_ch=obs_space, ch=MLP_CH, out_ch=action_space, block_num=6, is_relu=True)\n",
    "        self.sigma_param = nn.Parameter(torch.zeros(action_space, 1))\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        map_location=lambda storage, loc:storage\n",
    "        self.load_state_dict(torch.load(filename, map_location=map_location))\n",
    "        print('load model!')\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        print('save model!')\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        obs = torch.tensor(obs).float()\n",
    "        if self.is_gpu:\n",
    "            obs = obs.cuda()\n",
    "        \n",
    "        mu = self.net(obs)\n",
    "        shape = [1] * len(mu.shape)\n",
    "        shape[1] = -1\n",
    "        sigma = (self.sigma_param.view(shape) + torch.zeros_like(mu)).exp()\n",
    "        return (mu,sigma), state\n",
    "\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, is_gpu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_gpu = is_gpu\n",
    "        self.net = mlp_resblock_relu(in_ch=obs_space, ch=MLP_CH, out_ch=1, block_num=6, is_relu=False)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        map_location=lambda storage, loc:storage\n",
    "        self.load_state_dict(torch.load(filename, map_location=map_location))\n",
    "        print('load model!')\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        print('save model!')\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        obs = torch.tensor(obs).float()\n",
    "        if self.is_gpu:\n",
    "            obs = obs.cuda()\n",
    "        v = self.net(obs)\n",
    "\n",
    "        return v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(is_gpu = is_gpu)\n",
    "critic = Critic(is_gpu = is_gpu)\n",
    "\n",
    "load_path = None\n",
    "# load_path = 'save/ppo/exp1/ep09-actor.pth'\n",
    "# actor.load_model(load_path)\n",
    "# load_path = 'save/ppo/exp1/ep09-critic.pth'\n",
    "# critic.load_model(load_path)\n",
    "\n",
    "if is_gpu:\n",
    "    actor.cuda()\n",
    "    critic.cuda()\n",
    "\n",
    "    \n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "\n",
    "if load_path is None:\n",
    "    # orthogonal initialization\n",
    "    for m in actor_critic.modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.orthogonal_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            torch.nn.init.orthogonal_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(*logits):\n",
    "    return Independent(Normal(*logits), 1)\n",
    "\n",
    "\n",
    "action_space = gym.spaces.Box(0.0,1.0,(action_space,))\n",
    "\n",
    "if lr_decay:\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optim, lr_lambda=lambda epoch: lr_decay**(epoch-1)\n",
    "    )\n",
    "else:\n",
    "    lr_scheduler = None\n",
    "\n",
    "policy = ts.policy.PPOPolicy(actor, critic, optim, dist,\n",
    "        discount_factor=gamma, max_grad_norm=max_grad_norm,\n",
    "        eps_clip=eps_clip, vf_coef=vf_coef,\n",
    "        ent_coef=ent_coef, reward_normalization=rew_norm,\n",
    "        advantage_normalization=norm_adv, recompute_advantage=recompute_adv,\n",
    "        dual_clip=dual_clip, value_clip=value_clip,\n",
    "        gae_lambda=gae_lambda, action_space=action_space,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a73232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = DummyVectorEnv([lambda: OilSupply_Env() for _ in range(train_num)])\n",
    "test_envs = DummyVectorEnv([lambda: OilSupply_Env() for _ in range(test_num)]) \n",
    "\n",
    "# buffer = ts.data.PrioritizedVectorReplayBuffer(buffer_size, train_num, alpha=buffer_alpha, beta=buffer_beta)\n",
    "buffer = ts.data.VectorReplayBuffer(buffer_size, train_num)\n",
    "train_collector = ts.data.Collector(policy, train_envs, buffer)\n",
    "test_collector = ts.data.Collector(policy, test_envs)  # because DQN uses epsilon-greedy method\n",
    "train_collector.collect(n_episode=1)\n",
    "# # a,b = train_collector.collect(n_episode=1)\n",
    "\n",
    "def save_best_fn (policy):\n",
    "#     policy.actor.save_model('save/ppo/exp1/best-actor.pth')\n",
    "#     policy.critic.save_model('save/ppo/exp1/best-critic.pth')\n",
    "    pass\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.actor.save_model('save/ppo/exp1/ep%02d-actor.pth'%(epoch))\n",
    "    policy.critic.save_model('save/ppo/exp1/ep%02d-critic.pth'%(epoch))\n",
    "#     pass\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    pass\n",
    "    # policy.set_eps(eps_train)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews\n",
    "\n",
    "result = ts.trainer.onpolicy_trainer(\n",
    "        policy, train_collector, test_collector, epoch, step_per_epoch,\n",
    "        repeat_per_collect, test_num, batch_size,\n",
    "        episode_per_collect=episode_per_collect, save_best_fn =save_best_fn , logger=logger,\n",
    "        test_fn = test_fn, test_in_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Venido')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a388516f6b692c0a62646c30410c343eb82ea693e95e0ed4c54f075e0ce13e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
